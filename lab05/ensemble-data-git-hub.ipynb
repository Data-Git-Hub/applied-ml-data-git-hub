{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb673098",
   "metadata": {},
   "source": [
    "# Lab 5 - Ensembles \n",
    "Here is what we will do:\n",
    "1. Create a data set with two input features and a 3 category target\n",
    "2. Train a decision tree on the data set for a baseline\n",
    "3. Train 3 ensemble models\n",
    "    - Serial Ada Boosted DT\n",
    "    - Decision Forest\n",
    "    - Ensemble of DT, SVM, and NN\n",
    "3. Get model performance on train and test sets\n",
    "4. Create appropriate graphs\n",
    "5. Do a 10 fold cross validation with "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b676f033",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d625f",
   "metadata": {},
   "source": [
    "### Settings for the generated dataset\n",
    "This data set will be spirals with noise\n",
    "arms - This is the number of categories we will have\n",
    "turns - How many times we go around the spiral\n",
    "width - How wide is the spiral (percentage of the radius)\n",
    "noise - How much noise we will add to the data.\n",
    "size - The number of points to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda85eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = 3\n",
    "turns = 1.5\n",
    "width = 0.3\n",
    "noise = .25\n",
    "size = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353921fc",
   "metadata": {},
   "source": [
    "### The spirals will be based on the equation r=theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f21035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_arm(n):\n",
    "    print(\"Arm\", n)\n",
    "    points_per_arm = int(size/arms)\n",
    "    #Get points on the curve\n",
    "    theta_values = np.random.uniform(0, np.pi*turns, points_per_arm)\n",
    "    r_values = theta_values\n",
    "    target_values = [n] * points_per_arm\n",
    "    \n",
    "    #Add make the arm wider\n",
    "    lower_fraction = (1 - width)*theta_values\n",
    "    upper_fraction = (1 + width)*theta_values\n",
    "    difference = upper_fraction - lower_fraction\n",
    "    location = np.random.uniform(0, 1, points_per_arm)\n",
    "    theta_values = theta_values + lower_fraction + difference*location\n",
    "    \n",
    "    #offset angle \n",
    "    offset = n*2*np.pi/arms\n",
    "    print('Using offset', offset)\n",
    "    theta_values = theta_values + offset\n",
    "    \n",
    "    #draw the noise from a distribution centered on 0.0 with a standard deviation of noise\n",
    "    x_noise_values = np.random.normal(0, noise, points_per_arm)\n",
    "    y_noise_values = np.random.normal(0, noise, points_per_arm)\n",
    "    x_values = r_values * np.sin(theta_values) + x_noise_values\n",
    "    y_values = r_values * np.cos(theta_values) + y_noise_values\n",
    "    \n",
    "    \n",
    "    return x_values, y_values, target_values\n",
    "    \n",
    "\n",
    "def build_set():\n",
    "    data_x = np.array([])\n",
    "    data_y = np.array([])\n",
    "    data_t = np.array([])\n",
    "    print(\"Build set\")\n",
    "    for i in range(0,arms):\n",
    "        x, y, t = build_arm(i)\n",
    "        #print(data_x)\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        #print(t)\n",
    "        # add each arm to the data set\n",
    "        data_x=np.append(data_x, x)\n",
    "        data_y=np.append(data_y, y)\n",
    "        data_t=np.append(data_t, t)\n",
    "     # create a dictionary with each feature\n",
    "    d = {}\n",
    "    d[\"A\"] = data_x\n",
    "    d[\"B\"] = data_y\n",
    "    d[\"Class\"] = data_t\n",
    "\n",
    "    #print(d)\n",
    "\n",
    "    # Create the data frame from the dictionary\n",
    "    \n",
    "    dataframe = pd.DataFrame(data=d)\n",
    "    return dataframe\n",
    "    \n",
    "\n",
    "spiral = build_set()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#shuffle before plotting so we don't always overwrite with the same color\n",
    "shuffled = spiral.sample(frac=1)\n",
    "plt.scatter(shuffled['A'], shuffled['B'], c=shuffled['Class'], marker='+')\n",
    "\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3e97a",
   "metadata": {},
   "source": [
    "### Stratified Test/Train Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
    "for train_indices, test_indices in splitter.split(spiral, spiral['Class']):\n",
    "    train_set = spiral.iloc[train_indices]\n",
    "    test_set = spiral.iloc[test_indices]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcd0518",
   "metadata": {},
   "source": [
    "### A Helpful plotting function for showing true and false for the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba696942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot2FeatureBinaryConfusion(data_set, feature1Label, feature2Label, targetLabel, predicted, positive=1, negative=0):\n",
    "## assumes that the target is 0/1\n",
    "\n",
    "    \n",
    "    target = data_set[targetLabel]\n",
    "    \n",
    "    # Filter with feature 1\n",
    "    feature1_positives = np.ma.masked_where(target==negative, \n",
    "                     data_set[feature1Label])\n",
    "    true_positive = np.ma.masked_where(predicted!=target, feature1_positives)\n",
    "    false_negative = np.ma.masked_where(predicted==target, feature1_positives)\n",
    "    feature1_negatives = np.ma.masked_where(target==positive, \n",
    "                     data_set[feature1Label])\n",
    "    true_negative = np.ma.masked_where(predicted!=target, feature1_negatives)\n",
    "    false_positive = np.ma.masked_where(predicted==target,feature1_negatives)\n",
    "\n",
    "   \n",
    "    feature2 = data_set[feature2Label]\n",
    "    plt.scatter(true_positive, feature2, c='green', marker='o')\n",
    "    plt.scatter(false_positive, feature2, c='red', marker='^')\n",
    "    plt.scatter(true_negative, feature2, c='blue', marker='o')\n",
    "    plt.scatter(false_negative, feature2, c='orange', marker='^')\n",
    "\n",
    "\n",
    "\n",
    "    plt.xlabel(feature1Label)\n",
    "    plt.ylabel(feature2Label)\n",
    "    plt.legend(['True Positive', 'False Positive', 'True Negative', 'False Negative'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c57fbf",
   "metadata": {},
   "source": [
    "### Train and evaluate Decision Tree model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762571dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "  \n",
    "\n",
    "X = train_set[['A','B' ]]\n",
    "y = train_set['Class']\n",
    "\n",
    "X_test = test_set[['A','B']]\n",
    "y_test = test_set['Class']\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "tree_model.fit(X,y)\n",
    "\n",
    "y_pred = tree_model.predict(X)\n",
    "print('Results for decision tree on training data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('Accuracy is  ', accuracy_score(y, y_pred))\n",
    "print('Precision is ', precision_score(y, y_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y,y_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y, y_pred, average='weighted'))\n",
    "print()\n",
    "\n",
    "y_test_pred = tree_model.predict(X_test)\n",
    "print('Results for decision tree on test data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('Accuracy is  ', accuracy_score(y_test, y_test_pred))\n",
    "print('Precision is ', precision_score(y_test, y_test_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y_test,y_test_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y_test, y_test_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee18841",
   "metadata": {},
   "source": [
    "### Train and evaluate SVC model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3894dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "  \n",
    "\n",
    "X = train_set[['A','B' ]]\n",
    "y = train_set['Class']\n",
    "\n",
    "X_test = test_set[['A','B']]\n",
    "y_test = test_set['Class']\n",
    "\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X,y)\n",
    "\n",
    "y_pred = svc_model.predict(X)\n",
    "print('Results for svc on training data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('Accuracy is  ', accuracy_score(y, y_pred))\n",
    "print('Precision is ', precision_score(y, y_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y,y_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y, y_pred, average='weighted'))\n",
    "print()\n",
    "\n",
    "y_test_pred = svc_model.predict(X_test)\n",
    "print('Results for svc on test data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('Accuracy is  ', accuracy_score(y_test, y_test_pred))\n",
    "print('Precision is ', precision_score(y_test, y_test_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y_test,y_test_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y_test, y_test_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74130799",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "arm0_A = np.ma.masked_where(spiral['Class']!=0, \n",
    "                     spiral['A'])\n",
    "\n",
    "arm1_A = np.ma.masked_where(spiral['Class']!=1, \n",
    "                     spiral['A'])\n",
    "\n",
    "arm2_A = np.ma.masked_where(spiral['Class']!=2, \n",
    "                     spiral['A'])\n",
    "\n",
    "\n",
    "\n",
    "# get the values for the support vectors (the special instances)\n",
    "support_x = [x for (x,y) in svc_model.support_vectors_]\n",
    "support_y = [y for (x,y) in svc_model.support_vectors_]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b = spiral['B']\n",
    "plt.scatter(arm0_A, b, c='yellow', marker='o')\n",
    "plt.scatter(arm1_A, b, c='cyan', marker='o')\n",
    "plt.scatter(arm2_A, b, c='green', marker='o')\n",
    "plt.scatter(support_x, support_y, c='black', marker='+')\n",
    "\n",
    "plt.xlabel('A')\n",
    "plt.ylabel('B')\n",
    "plt.legend(['Arm0', 'Arm1', 'Arm2','Support vector'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f0ec7d",
   "metadata": {},
   "source": [
    "### Train and evaluate Neural Net model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaea955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "  \n",
    "\n",
    "X = train_set[['A','B' ]]\n",
    "y = train_set['Class']\n",
    "\n",
    "X_test = test_set[['A','B']]\n",
    "y_test = test_set['Class']\n",
    "\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(50, 25, 10),\n",
    "                         solver='lbfgs')\n",
    "nn_model.fit(X,y)\n",
    "\n",
    "y_pred = nn_model.predict(X)\n",
    "\n",
    "\n",
    "print('Results for NN on train data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('Accuracy is  ', accuracy_score(y, y_pred))\n",
    "print('Precision is ', precision_score(y, y_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y,y_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y, y_pred, average='weighted'))\n",
    "print()\n",
    "\n",
    "y_test_pred = nn_model.predict(X_test)\n",
    "print('Results for NN on test data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('Accuracy is  ', accuracy_score(y_test, y_test_pred))\n",
    "print('Precision is ', precision_score(y_test, y_test_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y_test,y_test_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y_test, y_test_pred, average='weighted'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b90ca4",
   "metadata": {},
   "source": [
    "# Results\n",
    "Basic results for our classification model to predict arm on the spiral data set\n",
    "\n",
    "| Model | Training Features | Acc Train | F1 Train |Acc Test | F1 Test |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "|Decision Tree|A,B|100.0|100.0|71.62 | 71.63 |\n",
    "|SVC|A,B|71.58|71.58|70.5|70.42|\n",
    "|MLP|A,B layers (50,25,10) lbfgs|80.06|80.13|77.87|77.91"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
