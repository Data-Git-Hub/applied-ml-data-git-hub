{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive California Housing Price Model 1990 US Census Bureau\n",
    "\n",
    "Author: Data-Git-Hub <br>\n",
    "GitHub Project Repository Link: https://github.com/Data-Git-Hub/applied-ml-data-git-hub <br>\n",
    "11 March 2025 <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "The U.S. Census Bureau collects extensive demographic, social, and economic data every ten years to analyze population trends and housing conditions. This data is gathered through mailed survey forms, in-person interviews, and follow-ups to account for non-responses. Despite its rigorous methodology, census data is subject to certain limitations, including sampling errors, misreporting, and undercounting, particularly in large, diverse states like California. The state's high population density, significant immigrant communities, and transient housing situations contribute to potential inaccuracies in data collection, affecting the precision of housing market analyses. <br>\n",
    "\n",
    "The California housing dataset within sklearn.model is derived from the 1990 U.S. Census and includes various economic and demographic factors that influence the housing market. Key columns in the dataset include median house values, median income, average household size, housing age, and geographical location. These variables provide a snapshot of the state's real estate landscape, allowing for predictive modeling and trend analysis. Understanding these attributes can help in assessing affordability, regional disparities, and socioeconomic influences on housing prices. <br>\n",
    "\n",
    "A closer examination of this dataset can reveal potential challenges in the California housing market. Issues such as affordability crises, income inequality, and housing shortages may emerge when analyzing the data trends. Given California's history of rapid urban expansion and economic booms, disparities in median incomes and housing costs could indicate broader problems, such as gentrification, rising rental burdens, and insufficient housing supply. Additionally, the limitations in the dataset may obscure certain critical aspects of the market, such as informal housing arrangements or fluctuations in demand due to migration patterns. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Python libraries are collections of pre-written code that provide specific functionalities, making programming more efficient and reducing the need to write code from scratch. These libraries cover a wide range of applications, including data analysis, machine learning, web development, and automation. Some libraries, such as os, sys, math, json, and datetime, come built-in with Python as part of its standard library, providing essential functions for file handling, system operations, mathematical computations, and data serialization. Other popular third-party libraries, like pandas, numpy, matplotlib, seaborn, and scikit-learn, must be installed separately and are widely used in data science and machine learning. The extensive availability of libraries in Python's ecosystem makes it a versatile and powerful programming language for various domains. <br>\n",
    "\n",
    "\n",
    "Pandas is a powerful data manipulation and analysis library that provides flexible data structures, such as DataFrames and Series. It is widely used for handling structured datasets, enabling easy data cleaning, transformation, and aggregation. Pandas is essential for data preprocessing in machine learning and statistical analysis. <br>\n",
    "https://pandas.pydata.org/docs/ <br>\n",
    "\n",
    "NumPy (Numerical Python) is a foundational library for numerical computing in Python. It provides support for large, multi-dimensional arrays and matrices, along with a comprehensive collection of mathematical functions to operate on these arrays efficiently. NumPy is a key component in scientific computing and machine learning. <br>\n",
    "https://numpy.org/doc/stable/ <br>\n",
    "\n",
    "Matplotlib is a widely used data visualization library that allows users to create static, animated, and interactive plots. It provides extensive tools for generating various chart types, including line plots, scatter plots, histograms, and bar charts, making it a critical library for exploratory data analysis. <br>\n",
    "https://matplotlib.org/stable/contents.html <br>\n",
    "\n",
    "Seaborn is a statistical data visualization library built on top of Matplotlib, designed for creating visually appealing and informative plots. It simplifies complex visualizations, such as heatmaps, violin plots, and pair plots, making it easier to identify patterns and relationships in datasets. <br>\n",
    "https://seaborn.pydata.org/ <br>\n",
    "\n",
    "Scikit-learn provides a variety of tools for machine learning, including data preprocessing, model selection, and evaluation. It contains essential functions for building predictive models and analyzing datasets. <br>\n",
    "\n",
    "ADD IPython.core.display <br>\n",
    "\n",
    "sklearn.datasets.fetch_california_housing: This function loads the California housing dataset from the 1990 U.S. Census Bureau report. It includes data on median home values, income levels, and other socioeconomic factors, making it useful for regression tasks. <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html <br>\n",
    "\n",
    "sklearn.model_selection.train_test_split: This function is used to split datasets into training and testing subsets, ensuring that models are evaluated on unseen data to prevent overfitting. It is a standard practice in machine learning workflows. <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html <br>\n",
    "\n",
    "sklearn.linear_model.LinearRegression: This class implements a simple linear regression model that predicts target values based on input features. It uses the least squares method to determine the best-fit line, making it a fundamental technique in regression analysis. <br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html <br>\n",
    "\n",
    "sklearn.metrics: This module provides various performance metrics for evaluating machine learning models. <br>\n",
    "\n",
    "* Root Mean Squared Error (RMSE): Measures the standard deviation of prediction errors, helping to understand model accuracy. <br>\n",
    "* Mean Absolute Error (MAE): Calculates the average absolute differences between predicted and actual values, indicating overall prediction accuracy. <br>\n",
    "* R² Score (R-Squared): Represents how well the independent variables explain the variance in the target variable. A higher R² value indicates a better-fitting model.<br>\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import pandas for data manipulation and analysis\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for creating static visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import seaborn for statistical data visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Import display for Jupyter-friendly output\n",
    "from IPython.display import display\n",
    "\n",
    "# Import the California housing dataset from sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Import train_test_split for splitting data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import LinearRegression for building a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import performance metrics for model evaluation\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a5fd6",
   "metadata": {},
   "source": [
    "### Project Outline\n",
    "The following approach will be used to analyze a dataset and build a predictive model systematically, ensuring that each step contributes to a deeper understanding of the data and enhances the accuracy of the final model. By following this structured workflow, we can make informed decisions about feature selection and model evaluation, ultimately improving predictive performance. <br>\n",
    "\n",
    "We begin with Section 1: Load and Explore the Data, where we retrieve the California housing dataset from sklearn.datasets and examine its structure. This initial exploration is crucial as it helps us understand the nature of the dataset, including the number of features, data types, missing values, and basic statistical summaries. Without this foundational step, we risk making incorrect assumptions about the data, which could lead to ineffective modeling. <br>\n",
    "\n",
    "Next, in Section 2: Visualize Feature Distributions, we focus on graphical representations of the dataset's features using histograms, box plots, and correlation heatmaps. Visualizing the data allows us to identify potential outliers, detect skewed distributions, and understand relationships between variables. This step provides insight into whether transformations or normalizations are necessary before feeding the data into a predictive model. <br>\n",
    "\n",
    "Moving forward, Section 3: Feature Selection and Justification involves carefully selecting which features should be included in our model. Not all features contribute equally to prediction accuracy; some may be redundant or introduce noise. Using statistical methods such as correlation analysis and variance thresholds, we determine which variables are most relevant for predicting house prices. Proper feature selection enhances model efficiency and prevents overfitting. <br>\n",
    "\n",
    "Finally, in Section 4: Train a Linear Regression Model, we implement and train a simple linear regression model using the selected features. This step involves splitting the dataset into training and testing subsets, fitting the model, and evaluating its performance using metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R-squared (R²). By doing so, we assess how well our model generalizes to unseen data and identify areas for potential improvement. <br>\n",
    "\n",
    "Following this structured approach ensures a logical progression from data understanding to model development, allowing for a well-informed, data-driven approach to predictive modeling. <br>\n",
    "\n",
    "Section 1. Load and Explore the Data <br>\n",
    "Section 2. Visualize Feature Distributions <br>\n",
    "Section 3. Feature Selection and Justification <br>\n",
    "Section 4. Train a Linear Regression Model <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1. Load and Explore the Data\n",
    "\n",
    "1.1 Load the dataset and display the first 10 rows <br>\n",
    "\n",
    "Load the California housing dataset directly from `scikit-learn`. <br>\n",
    "- The `fetch_california_housing` function returns a dictionary-like object with the data. <br>\n",
    "- Convert it into a pandas DataFrame. <br>\n",
    "- Display just the first 10 rows using `head()`. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing Data Preview:\n",
      "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude   Longitude  MedHouseVal\n",
      "0 8.325200 41.000000  6.984127   1.023810  322.000000  2.555556 37.880000 -122.230000     4.526000\n",
      "1 8.301400 21.000000  6.238137   0.971880 2401.000000  2.109842 37.860000 -122.220000     3.585000\n",
      "2 7.257400 52.000000  8.288136   1.073446  496.000000  2.802260 37.850000 -122.240000     3.521000\n",
      "3 5.643100 52.000000  5.817352   1.073059  558.000000  2.547945 37.850000 -122.250000     3.413000\n",
      "4 3.846200 52.000000  6.281853   1.081081  565.000000  2.181467 37.850000 -122.250000     3.422000\n",
      "5 4.036800 52.000000  4.761658   1.103627  413.000000  2.139896 37.850000 -122.250000     2.697000\n",
      "6 3.659100 52.000000  4.931907   0.951362 1094.000000  2.128405 37.840000 -122.250000     2.992000\n",
      "7 3.120000 52.000000  4.797527   1.061824 1157.000000  1.788253 37.840000 -122.250000     2.414000\n",
      "8 2.080400 42.000000  4.294118   1.117647 1206.000000  2.026891 37.840000 -122.260000     2.267000\n",
      "9 3.691200 52.000000  4.970588   0.990196 1551.000000  2.172269 37.840000 -122.250000     2.611000 \n",
      "\n",
      "Missing values in California Housing Data:\n",
      "MedInc         0\n",
      "HouseAge       0\n",
      "AveRooms       0\n",
      "AveBedrms      0\n",
      "Population     0\n",
      "AveOccup       0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "MedHouseVal    0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "data_frame = data.frame  # Convert to Pandas DataFrame\n",
    "\n",
    "# Set pandas display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display first 10 rows of the dataset\n",
    "print(\"California Housing Data Preview:\")\n",
    "print(data_frame.head(10).to_string(), \"\\n\")\n",
    "\n",
    "# Check for missing values in the dataset and ensure the output always shows\n",
    "missing_values = data_frame.isna().sum()\n",
    "\n",
    "print(\"Missing values in California Housing Data:\")\n",
    "print(missing_values.to_string(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Check for missing values and display summary statistics <br>\n",
    "\n",
    "In the cell below: <br>\n",
    "1. Use `info()` to check data types and missing values. <br>\n",
    "2. Use `describe()` to see summary statistics. <br>\n",
    "3. Use `isnull().sum()` to identify missing values in each column. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       20640 non-null  float64\n",
      " 1   HouseAge     20640 non-null  float64\n",
      " 2   AveRooms     20640 non-null  float64\n",
      " 3   AveBedrms    20640 non-null  float64\n",
      " 4   Population   20640 non-null  float64\n",
      " 5   AveOccup     20640 non-null  float64\n",
      " 6   Latitude     20640 non-null  float64\n",
      " 7   Longitude    20640 non-null  float64\n",
      " 8   MedHouseVal  20640 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.870671</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>5.429000</td>\n",
       "      <td>1.096675</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>3.070655</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>-119.569704</td>\n",
       "      <td>2.068558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.899822</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2.474173</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>10.386050</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>2.003532</td>\n",
       "      <td>1.153956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.563400</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.440716</td>\n",
       "      <td>1.006079</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>2.429741</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.534800</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.229129</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>2.818116</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.490000</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.743250</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.052381</td>\n",
       "      <td>1.099526</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>3.282261</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>2.647250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MedInc     HouseAge     AveRooms    AveBedrms   Population     AveOccup     Latitude    Longitude  MedHouseVal\n",
       "count 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000 20640.000000\n",
       "mean      3.870671    28.639486     5.429000     1.096675  1425.476744     3.070655    35.631861  -119.569704     2.068558\n",
       "std       1.899822    12.585558     2.474173     0.473911  1132.462122    10.386050     2.135952     2.003532     1.153956\n",
       "min       0.499900     1.000000     0.846154     0.333333     3.000000     0.692308    32.540000  -124.350000     0.149990\n",
       "25%       2.563400    18.000000     4.440716     1.006079   787.000000     2.429741    33.930000  -121.800000     1.196000\n",
       "50%       3.534800    29.000000     5.229129     1.048780  1166.000000     2.818116    34.260000  -118.490000     1.797000\n",
       "75%       4.743250    37.000000     6.052381     1.099526  1725.000000     3.282261    37.710000  -118.010000     2.647250\n",
       "max      15.000100    52.000000   141.909091    34.066667 35682.000000  1243.333333    41.950000  -114.310000     5.000010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in the Dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MedInc         0\n",
       "HouseAge       0\n",
       "AveRooms       0\n",
       "AveBedrms      0\n",
       "Population     0\n",
       "AveOccup       0\n",
       "Latitude       0\n",
       "Longitude      0\n",
       "MedHouseVal    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Duplicate Rows: 0\n",
      "\n",
      "Columns with a Single Unique Value: []\n",
      "\n",
      "\n",
      "Checking for Zero or Negative Values in Key Columns:\n",
      "MedInc: 0 values <= 0\n",
      "HouseAge: 0 values <= 0\n",
      "AveRooms: 0 values <= 0\n",
      "AveBedrms: 0 values <= 0\n",
      "Population: 0 values <= 0\n",
      "AveOccup: 0 values <= 0\n",
      "\n",
      "Data exploration and cleaning checks completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the California housing dataset\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "data_frame = data.frame  # Convert to Pandas DataFrame\n",
    "\n",
    "# Set pandas display options to prevent truncation\n",
    "pd.set_option('display.max_rows', 500)  # Adjust row display limit if necessary\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)  # Format float precision\n",
    "\n",
    "# Display dataset information\n",
    "print(\"Dataset Information:\")\n",
    "display(data_frame.info())  # Using display() to prevent truncation\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(data_frame.describe())  # Ensure all summary statistics are fully visible\n",
    "\n",
    "print(\"\\nMissing Values in the Dataset:\")\n",
    "display(data_frame.isnull().sum())  # Ensure all missing values are displayed\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = data_frame.duplicated().sum()\n",
    "print(f\"\\nNumber of Duplicate Rows: {duplicates}\\n\")\n",
    "\n",
    "# Check for any constant (single-value) columns that may not be useful\n",
    "constant_columns = [col for col in data_frame.columns if data_frame[col].nunique() == 1]\n",
    "print(f\"Columns with a Single Unique Value: {constant_columns}\\n\")\n",
    "\n",
    "# Check for zero or negative values in relevant columns\n",
    "print(\"\\nChecking for Zero or Negative Values in Key Columns:\")\n",
    "for col in [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\"]:\n",
    "    zero_negative_count = (data_frame[col] <= 0).sum()\n",
    "    print(f\"{col}: {zero_negative_count} values <= 0\")\n",
    "\n",
    "print(\"\\nData exploration and cleaning checks completed!\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c001a63",
   "metadata": {},
   "source": [
    "### Conclusions - Section 1\n",
    "- Analysis of Data: <br> \n",
    "\n",
    "1) How many data instances (also called data records or data rows) are there? <br>\n",
    "   20,640 data records. <br>\n",
    "\n",
    "2) How many features (also columns or attributes) are there? <br>\n",
    "   There is a total of 9 features. <br>\n",
    "\n",
    "3) What are the names of the features? (\"Feature\" is used most often in ML projects.) <br>\n",
    "   The feature names are MedInc, HouseAge, AveRooms, AveBedrms, Population, AveOccup, Latitude, Longitude, and MedHouseVal. <br>\n",
    "\n",
    "4) Which features are numeric? <br>\n",
    "   All features are numeric. <br>\n",
    "\n",
    "5) Which features are categorical (non-numeric)? <br>\n",
    "   None. <br>\n",
    "\n",
    "6) Are there any missing values? How should they be handled? Should we delete a sparsely populated column? Delete an incomplete data row? Substitute with a different value? <br>\n",
    "   Through the previewing and cleaning process of the data set, all the column met the measurement of appropriate data for usage in ML. <br>\n",
    "\n",
    "7) What else do you notice about the dataset? Are there any data issues? <br>\n",
    "   The dataset is well-structured and clean, with no missing values, making it relatively easy to work with. All features are numeric, meaning no categorical encoding is required. However, some aspects of the data warrant closer examination. <br>\n",
    "\n",
    "    One key observation is the wide range of values in certain features, such as Population. This could suggest potential outliers or skewness, which may impact model performance. Extreme values might distort statistical summaries and predictions, so further analysis—such as visualizing distributions using histograms or box plots—would help determine whether data transformations (e.g., log scaling) are necessary. <br>\n",
    "\n",
    "    Another consideration is feature relationships and multicollinearity. Since variables like AveRooms, AveBedrms, and Population are interrelated, high correlations between them could affect the stability of regression models. Conducting a correlation analysis will help identify redundant features, ensuring the model does not suffer from overfitting or biased coefficients. <br>\n",
    "\n",
    "    Additionally, while the dataset provides aggregated neighborhood-level data, it does not contain individual household details. This means any insights derived must be interpreted at a regional level rather than for specific properties. This aggregation could hide localized housing trends and introduce some loss of granularity in predictions. <br>\n",
    "\n",
    "    Overall, while the dataset is clean and well-prepared for analysis, further outlier detection, correlation checks, and potential transformations could enhance its usability for predictive modeling. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2. Visualize Feature Distributions <br>\n",
    "\n",
    "2.1 Create histograms, boxplots, and scatterplots <br>\n",
    "\n",
    "- Create histograms for all numeric features using `data_frame.hist()` with 30 bins. <br>\n",
    "- Create a boxenplots using `sns.boxenplot()`. <br>\n",
    "- Create scatter plots using `sns.pairplot()`. <br>\n",
    "\n",
    "First, histograms <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4a38b",
   "metadata": {},
   "source": [
    "Generate one Boxenplot for each column (good for large datasets) <br>\n",
    "\n",
    "Example code: <br>\n",
    "\n",
    "for column in data_frame.columns:\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxenplot(data=data_frame[column])\n",
    "    plt.title(f'Boxenplot for {column}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9f0158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad19958",
   "metadata": {},
   "source": [
    "Third - Scatter Plots <br>\n",
    "\n",
    "Generate all Scatter plots (there is a LOT of data, so this will take a while) <br>\n",
    "\n",
    "Comment out after analysis to speed up the notebook. <br>\n",
    "\n",
    "Example code: \n",
    "\n",
    "sns.pairplot(data_frame)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "014c64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d7146",
   "metadata": {},
   "source": [
    "### Section 3. Feature Selection and Justification <br>\n",
    "- 3.1 Choose two input features for predicting the target.\n",
    "- 3.2 Justify your selection with reasoning.\n",
    "\n",
    "Analysis: Why did you choose these features? How might they impact predictions?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3. Feature Selection and Justification <br>\n",
    "\n",
    "3.1 Choose two input features for predicting the target <br>\n",
    "\n",
    "- Select `MedInc` and `AveRooms` as predictors. <br>\n",
    "- Select `MedHouseVal` as the target variable. <br>\n",
    "\n",
    "In the following, <br> \n",
    "X is capitalized because it represents a matrix (consistent with mathematical notation). <br>\n",
    "y is lowercase because it represents a vector (consistent with mathematical notation). <br>\n",
    "\n",
    "\n",
    "First: <br>\n",
    "- Create a list of contributing features and the target variable <br>\n",
    "- Define the target feature string (the variable we want to predict) <br>\n",
    "- Define the input DataFrame <br>\n",
    "- Define the output DataFrame <br>\n",
    "\n",
    "\n",
    "Example code:\n",
    "\n",
    "features: list = ['MedInc', 'AveRooms']\n",
    "\n",
    "target: str = 'MedHouseVal'\n",
    "\n",
    "df_X = data_frame[features]\n",
    "\n",
    "df_y = data_frame[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b759e835",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Markdown Cell.\n",
    "\n",
    "\n",
    "## Section 4. Train a Linear Regression Model\n",
    "### 4.1 Split the data\n",
    "Split the dataset into training and test sets (80% train / 20% test).\n",
    "\n",
    "Call train_test_split() by passing in: \n",
    "\n",
    "- df_X – Feature matrix (input data) as a pandas DataFrame\n",
    "- y – Target values as a pandas Series\n",
    "- test_size – Fraction of data to use for testing (e.g., 0.1 = 10%)\n",
    "- random_state – Seed value for reproducible splits\n",
    "\n",
    "We'll get back four return values:\n",
    "\n",
    "- X_train – Training set features (DataFrame)\n",
    "- X_test – Test set features (DataFrame)\n",
    "- y_train – Training set target values (Series)\n",
    "- y_test – Test set target values (Series)\n",
    "\n",
    "\n",
    "Example code:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_X, df_y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Markdown Cell.\n",
    "\n",
    "### 4.2 Train the model\n",
    "Create and fit a `LinearRegression` model.\n",
    "\n",
    "LinearRegression – A class from sklearn.linear_model that creates a linear regression model.\n",
    "\n",
    "model – An instance of the LinearRegression model. This object will store the learned coefficients and intercept after training.\n",
    "\n",
    "fit() – Trains the model by finding the best-fit line for the training data using the Ordinary Least Squares (OLS) method.\n",
    "\n",
    "X_train – The input features used to train the model.\n",
    "\n",
    "y_train – The target values used to train the model.\n",
    "\n",
    "\n",
    "Example code:\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5676c419",
   "metadata": {},
   "source": [
    "This is a Markdown Cell.\n",
    "\n",
    "Make predictions for the test set.\n",
    "\n",
    "The model.predict() method applies this equation to the X test data to compute predicted values.\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred contains all the predicted values for all the rows in X_test based on the linear regression model.\n",
    "\n",
    "\n",
    "Example code:\n",
    "\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb721051",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Markdown Cell. \n",
    "\n",
    "### 4.3 Report R^2, MAE, RMSE\n",
    "Evaluate the model using R^2, MAE, and RMSE.\n",
    "\n",
    "First:\n",
    "\n",
    "- Coefficient of Determination (R^2) - This tells you how well the model explains the variation in the target variable. A value close to 1 means the model fits the data well; a value close to 0 means the model doesn’t explain the variation well.\n",
    "\n",
    "\n",
    "Example code:\n",
    "  \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'R²: {r2:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351f529b",
   "metadata": {},
   "source": [
    "\n",
    "This is a Markdown Cell\n",
    "\n",
    "Second:\n",
    "\n",
    "- Mean Absolute Error (MAE) - This is the average of the absolute differences between the predicted values and the actual values. A smaller value means the model’s predictions are closer to the actual values.\n",
    "\n",
    "\n",
    "Example code:\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'MAE: {mae:.2f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82df3db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf1fd0",
   "metadata": {},
   "source": [
    "\n",
    "This is a Markdown Cell\n",
    "\n",
    "Third:\n",
    "\n",
    "- Root Mean Squared Error (RMSE) - This is the square root of the average of the squared differences between the predicted values and the actual values. It gives a sense of how far the predictions are from the actual values, with larger errors having more impact.\n",
    "\n",
    "Example code:\n",
    "\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'RMSE: {rmse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eda3368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a Python cell. \n",
    "# Put your comments and code here.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b399ae27",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
